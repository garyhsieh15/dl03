{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LeNet5_NEW.ipynb","provenance":[],"authorship_tag":"ABX9TyPd0aSbxXKXuAthuZGP3Tdb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sq6uLN8H1dHQ","executionInfo":{"status":"ok","timestamp":1619369188289,"user_tz":-480,"elapsed":22640,"user":{"displayName":"N68091017謝孝勇","photoUrl":"","userId":"07716993591171598981"}},"outputId":"c1e9dbd9-d33b-4d13-c7ed-a44216aeb2b3"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NzhsdsDG10d7","executionInfo":{"status":"ok","timestamp":1619369192554,"user_tz":-480,"elapsed":2993,"user":{"displayName":"N68091017謝孝勇","photoUrl":"","userId":"07716993591171598981"}},"outputId":"3c51ef90-3a55-4ef7-c4b5-32978b1aab8a"},"source":["%cd /content/drive/MyDrive/work/NCKU/10902/dl/HW03/dl03/"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/work/NCKU/10902/dl/HW03/dl03\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mX_J_HAFzzeJ"},"source":["**1. train processing**\n","\n"]},{"cell_type":"code","metadata":{"id":"17skskZK8ooc","executionInfo":{"status":"ok","timestamp":1619369196087,"user_tz":-480,"elapsed":2226,"user":{"displayName":"N68091017謝孝勇","photoUrl":"","userId":"07716993591171598981"}}},"source":["import os\n","import random\n","import cv2\n","import numpy as np\n","from lenet import LeNet\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"k90Q7Pf_3QHC","executionInfo":{"status":"ok","timestamp":1619369198733,"user_tz":-480,"elapsed":1223,"user":{"displayName":"N68091017謝孝勇","photoUrl":"","userId":"07716993591171598981"}}},"source":["# 取得檔案名稱, 將檔案名稱的第一個字元, 作為標籤(答案)的方式, 最後return list labels.\n","def get_training_data(data_dir):\n","    images = []\n","    labels = []\n","    files = os.listdir(data_dir)\n","    #print(\" -- files: \\n\", files)\n","    random.shuffle(files)\n","    #i = 0\n","    for f in files:\n","        # path + file name\n","        img = cv2.imread(os.path.join(data_dir, f), cv2.IMREAD_GRAYSCALE)\n","        #img = cv2.imread(os.path.join(data_dir, f))\n","        #print(\" -- img type: \", type(img))\n","        #print(\" -- os path join: \\n\", os.path.join(data_dir, f))\n","        print(\" -- img shape: \\n: \", img.shape)\n","        img = cv2.resize(img, (32, 32))\n","        #img = cv2.resize(img, (320, 320))\n","        #img = cv2.resize(img, (768, 384))\n","        #img = cv2.resize(img, (768, 768))\n","        #print(\" -- img resize, img shape: \\n\", img, img.shape)\n","        img = img.astype(np.float32).reshape(32, 32, 1) / 255.0\n","        #img = img.astype(np.float32).reshape(320, 320, 1) / 255.0\n","        #img = img.astype(np.float32).reshape(768, 768, 1) / 255.0\n","        #print(\" -- img astype and img shape: \\n\", img, img.shape)\n","        #print(\" -- img astype * 255: \\n\", img * 255)\n","        images.append(img)\n","        #print(\"images type: \", type(images))\n","        \n","        \"\"\"\n","        if i <= 1:\n","            print(\" -- img astype: \\n\", img)\n","            print(\" -- image and i value: \\n\", images, i)\n","            i += 1\n","        \"\"\"\n","        # bear -> 0,    leopard    -> 1,    tiger   -> 2\n","        # dog  -> 3,    cat.       -> 4,    lion.   -> 5\n","        # fox. -> 6,    polar_bear -> 7,    meerkat -> 8\n","        # wolf -> 9,    \n","\n","        # use first letter to define label's answer.\n","        #num = int(f[0])\n","        #print(\" -- f \\n\", f)\n","        if \"bear\" in f:\n","            num = 0\n","        elif \"leopard\" in f:\n","            num = 1\n","        elif \"tiger\" in f:\n","            num = 2\n","        elif \"dog\" in f:\n","            num = 3\n","        elif \"cat\" in f:\n","            num = 4\n","        elif \"lion\" in f:\n","            num = 5\n","        elif \"fox\" in f:\n","            num = 6\n","        elif \"polar_bear\" in f:\n","            num = 7\n","        elif \"meerkat\" in f:\n","            num = 8\n","        elif \"wolf\" in f:\n","            num = 9\n","          \n","        #print(\" -- label[1]: \\n\", label_type[1])\n","        label = np.zeros(10, dtype=np.float32)\n","        # num表示答案, 將那個答案填入升起的flag = 1, 作為標籤答案的表示法.\n","        label[num] = 1\n","        labels.append(label)\n","        #print(\" -- labels: \\n\", labels)\n","        # ---------------------------------------------------------\n","        \"\"\"\n","        i += 1\n","        if i == 1:\n","            break\n","        \"\"\"\n","        # ---------------------------------------------------------\n","    return (np.array(images), np.array(labels))\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqaPVLA33SDN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619369294992,"user_tz":-480,"elapsed":93136,"user":{"displayName":"N68091017謝孝勇","photoUrl":"","userId":"07716993591171598981"}},"outputId":"20b759e4-06a6-4824-9f91-8979b0d52342"},"source":["if __name__ == '__main__':\n","    x, y = get_training_data(\"./images/train/\")\n","    lenet = LeNet()\n","    lenet.train(x, y)\n","    lenet.save(\"./lenet.npy\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":[" -- img shape: \n",":  (256, 392)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (384, 256)\n"," -- img shape: \n",":  (390, 256)\n"," -- img shape: \n",":  (256, 364)\n"," -- img shape: \n",":  (256, 320)\n"," -- img shape: \n",":  (256, 334)\n"," -- img shape: \n",":  (384, 256)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 359)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 384)\n"," -- img shape: \n",":  (272, 256)\n"," -- img shape: \n",":  (337, 256)\n"," -- img shape: \n",":  (256, 263)\n"," -- img shape: \n",":  (256, 384)\n"," -- img shape: \n",":  (256, 310)\n"," -- img shape: \n",":  (256, 319)\n"," -- img shape: \n",":  (256, 371)\n"," -- img shape: \n",":  (358, 256)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (387, 256)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (341, 256)\n"," -- img shape: \n",":  (256, 384)\n"," -- img shape: \n",":  (341, 256)\n"," -- img shape: \n",":  (259, 256)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 280)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 317)\n"," -- img shape: \n",":  (256, 375)\n"," -- img shape: \n",":  (384, 256)\n"," -- img shape: \n",":  (256, 369)\n"," -- img shape: \n",":  (256, 375)\n"," -- img shape: \n",":  (341, 256)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 384)\n"," -- img shape: \n",":  (256, 369)\n"," -- img shape: \n",":  (256, 392)\n"," -- img shape: \n",":  (261, 256)\n"," -- img shape: \n",":  (341, 256)\n"," -- img shape: \n",":  (256, 319)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 330)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 256)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 359)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 256)\n"," -- img shape: \n",":  (256, 320)\n"," -- img shape: \n",":  (256, 384)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 390)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 332)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 382)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 375)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (307, 256)\n"," -- img shape: \n",":  (263, 256)\n"," -- img shape: \n",":  (256, 337)\n"," -- img shape: \n",":  (341, 256)\n"," -- img shape: \n",":  (256, 256)\n"," -- img shape: \n",":  (256, 343)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 335)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (384, 256)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 343)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 392)\n"," -- img shape: \n",":  (320, 256)\n"," -- img shape: \n",":  (256, 256)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 373)\n"," -- img shape: \n",":  (256, 307)\n"," -- img shape: \n",":  (256, 366)\n"," -- img shape: \n",":  (256, 388)\n"," -- img shape: \n",":  (256, 366)\n"," -- img shape: \n",":  (256, 386)\n"," -- img shape: \n",":  (256, 341)\n"," -- img shape: \n",":  (256, 334)\n"," -- img shape: \n",":  (256, 356)\n"," -- img shape: \n",":  (256, 371)\n","step 0: loss=3.436935, accuracy=0.0900, lr=0.0003\n","step 1: loss=3.426820, accuracy=0.0900, lr=0.0002997\n","step 2: loss=3.409508, accuracy=0.0900, lr=0.0002994\n","step 3: loss=3.386955, accuracy=0.0900, lr=0.000299101\n","step 4: loss=3.361818, accuracy=0.0900, lr=0.000298802\n","step 5: loss=3.335804, accuracy=0.1000, lr=0.000298503\n","step 6: loss=3.310709, accuracy=0.0900, lr=0.000298204\n","step 7: loss=3.286552, accuracy=0.1000, lr=0.000297906\n","step 8: loss=3.264658, accuracy=0.1100, lr=0.000297608\n","step 9: loss=3.246311, accuracy=0.1200, lr=0.000297311\n","step 10: loss=3.231403, accuracy=0.1400, lr=0.000297013\n","step 11: loss=3.219302, accuracy=0.1600, lr=0.000296716\n","step 12: loss=3.210438, accuracy=0.1900, lr=0.00029642\n","step 13: loss=3.203588, accuracy=0.1800, lr=0.000296123\n","step 14: loss=3.198379, accuracy=0.1800, lr=0.000295827\n","step 15: loss=3.193700, accuracy=0.1800, lr=0.000295531\n","step 16: loss=3.189959, accuracy=0.1900, lr=0.000295236\n","step 17: loss=3.186756, accuracy=0.1900, lr=0.000294941\n","step 18: loss=3.183145, accuracy=0.1900, lr=0.000294646\n","step 19: loss=3.179688, accuracy=0.1900, lr=0.000294351\n","step 20: loss=3.176537, accuracy=0.1900, lr=0.000294057\n","step 21: loss=3.173498, accuracy=0.2000, lr=0.000293763\n","step 22: loss=3.170155, accuracy=0.2000, lr=0.000293469\n","step 23: loss=3.166867, accuracy=0.2000, lr=0.000293175\n","step 24: loss=3.163654, accuracy=0.2000, lr=0.000292882\n","step 25: loss=3.160526, accuracy=0.2000, lr=0.000292589\n","step 26: loss=3.157275, accuracy=0.2000, lr=0.000292297\n","step 27: loss=3.153844, accuracy=0.2000, lr=0.000292004\n","step 28: loss=3.150509, accuracy=0.2000, lr=0.000291712\n","step 29: loss=3.147156, accuracy=0.2000, lr=0.000291421\n","step 30: loss=3.143804, accuracy=0.2000, lr=0.000291129\n","step 31: loss=3.140643, accuracy=0.2000, lr=0.000290838\n","step 32: loss=3.137529, accuracy=0.2000, lr=0.000290547\n","step 33: loss=3.134357, accuracy=0.2000, lr=0.000290257\n","step 34: loss=3.131182, accuracy=0.2000, lr=0.000289967\n","step 35: loss=3.128238, accuracy=0.2000, lr=0.000289677\n","step 36: loss=3.125369, accuracy=0.2000, lr=0.000289387\n","step 37: loss=3.122433, accuracy=0.2000, lr=0.000289097\n","step 38: loss=3.119618, accuracy=0.2000, lr=0.000288808\n","step 39: loss=3.116840, accuracy=0.2000, lr=0.00028852\n","step 40: loss=3.114080, accuracy=0.2000, lr=0.000288231\n","step 41: loss=3.111164, accuracy=0.2000, lr=0.000287943\n","step 42: loss=3.108117, accuracy=0.2000, lr=0.000287655\n","step 43: loss=3.105002, accuracy=0.2000, lr=0.000287367\n","step 44: loss=3.101920, accuracy=0.2000, lr=0.00028708\n","step 45: loss=3.098870, accuracy=0.2000, lr=0.000286793\n","step 46: loss=3.095870, accuracy=0.2000, lr=0.000286506\n","step 47: loss=3.092941, accuracy=0.2000, lr=0.000286219\n","step 48: loss=3.090080, accuracy=0.2000, lr=0.000285933\n","step 49: loss=3.087248, accuracy=0.2000, lr=0.000285647\n","step 50: loss=3.084385, accuracy=0.2000, lr=0.000285362\n","step 51: loss=3.081513, accuracy=0.2000, lr=0.000285076\n","step 52: loss=3.078501, accuracy=0.2000, lr=0.000284791\n","step 53: loss=3.075500, accuracy=0.2000, lr=0.000284506\n","step 54: loss=3.072513, accuracy=0.2000, lr=0.000284222\n","step 55: loss=3.069553, accuracy=0.2000, lr=0.000283938\n","step 56: loss=3.066618, accuracy=0.2000, lr=0.000283654\n","step 57: loss=3.063639, accuracy=0.2100, lr=0.00028337\n","step 58: loss=3.060678, accuracy=0.2100, lr=0.000283087\n","step 59: loss=3.057670, accuracy=0.2100, lr=0.000282804\n","step 60: loss=3.054655, accuracy=0.2100, lr=0.000282521\n","step 61: loss=3.051666, accuracy=0.2100, lr=0.000282238\n","step 62: loss=3.048742, accuracy=0.2100, lr=0.000281956\n","step 63: loss=3.045835, accuracy=0.2100, lr=0.000281674\n","step 64: loss=3.042942, accuracy=0.2100, lr=0.000281392\n","step 65: loss=3.040015, accuracy=0.2100, lr=0.000281111\n","step 66: loss=3.036991, accuracy=0.2100, lr=0.00028083\n","step 67: loss=3.033946, accuracy=0.2100, lr=0.000280549\n","step 68: loss=3.030856, accuracy=0.2100, lr=0.000280269\n","step 69: loss=3.027817, accuracy=0.2100, lr=0.000279988\n","step 70: loss=3.024746, accuracy=0.2100, lr=0.000279708\n","step 71: loss=3.021620, accuracy=0.2100, lr=0.000279429\n","step 72: loss=3.018468, accuracy=0.2100, lr=0.000279149\n","step 73: loss=3.015306, accuracy=0.2100, lr=0.00027887\n","step 74: loss=3.012193, accuracy=0.2100, lr=0.000278591\n","step 75: loss=3.009104, accuracy=0.2100, lr=0.000278313\n","step 76: loss=3.006053, accuracy=0.2100, lr=0.000278034\n","step 77: loss=3.003107, accuracy=0.2100, lr=0.000277756\n","step 78: loss=3.000332, accuracy=0.2100, lr=0.000277478\n","step 79: loss=2.997544, accuracy=0.2100, lr=0.000277201\n","step 80: loss=2.994662, accuracy=0.2100, lr=0.000276924\n","step 81: loss=2.991591, accuracy=0.2100, lr=0.000276647\n","step 82: loss=2.988394, accuracy=0.2100, lr=0.00027637\n","step 83: loss=2.985182, accuracy=0.2100, lr=0.000276094\n","step 84: loss=2.981853, accuracy=0.2100, lr=0.000275818\n","step 85: loss=2.978468, accuracy=0.2200, lr=0.000275542\n","step 86: loss=2.975007, accuracy=0.2200, lr=0.000275266\n","step 87: loss=2.971550, accuracy=0.2200, lr=0.000274991\n","step 88: loss=2.968095, accuracy=0.2200, lr=0.000274716\n","step 89: loss=2.964670, accuracy=0.2200, lr=0.000274441\n","step 90: loss=2.961200, accuracy=0.2200, lr=0.000274167\n","step 91: loss=2.957732, accuracy=0.2200, lr=0.000273893\n","step 92: loss=2.954373, accuracy=0.2200, lr=0.000273619\n","step 93: loss=2.951076, accuracy=0.2200, lr=0.000273345\n","step 94: loss=2.947703, accuracy=0.2200, lr=0.000273072\n","step 95: loss=2.944208, accuracy=0.2200, lr=0.000272799\n","step 96: loss=2.940593, accuracy=0.2200, lr=0.000272526\n","step 97: loss=2.936965, accuracy=0.2200, lr=0.000272254\n","step 98: loss=2.933317, accuracy=0.2200, lr=0.000271981\n","step 99: loss=2.929659, accuracy=0.2200, lr=0.000271709\n","step 100: loss=2.925972, accuracy=0.2200, lr=0.000271438\n","step 101: loss=2.922184, accuracy=0.2200, lr=0.000271166\n","step 102: loss=2.918303, accuracy=0.2200, lr=0.000270895\n","step 103: loss=2.914292, accuracy=0.2200, lr=0.000270624\n","step 104: loss=2.910267, accuracy=0.2200, lr=0.000270354\n","step 105: loss=2.906204, accuracy=0.2200, lr=0.000270083\n","step 106: loss=2.902164, accuracy=0.2200, lr=0.000269813\n","step 107: loss=2.898078, accuracy=0.2200, lr=0.000269543\n","step 108: loss=2.893913, accuracy=0.2300, lr=0.000269274\n","step 109: loss=2.889689, accuracy=0.2300, lr=0.000269004\n","step 110: loss=2.885474, accuracy=0.2400, lr=0.000268735\n","step 111: loss=2.881230, accuracy=0.2500, lr=0.000268467\n","step 112: loss=2.876951, accuracy=0.2500, lr=0.000268198\n","step 113: loss=2.872631, accuracy=0.2600, lr=0.00026793\n","step 114: loss=2.868291, accuracy=0.2900, lr=0.000267662\n","step 115: loss=2.863715, accuracy=0.2900, lr=0.000267394\n","step 116: loss=2.858963, accuracy=0.2900, lr=0.000267127\n","step 117: loss=2.854127, accuracy=0.2900, lr=0.00026686\n","step 118: loss=2.849271, accuracy=0.2900, lr=0.000266593\n","step 119: loss=2.844312, accuracy=0.2900, lr=0.000266326\n","step 120: loss=2.839217, accuracy=0.2900, lr=0.00026606\n","step 121: loss=2.834100, accuracy=0.2900, lr=0.000265794\n","step 122: loss=2.828965, accuracy=0.2800, lr=0.000265528\n","step 123: loss=2.823753, accuracy=0.2800, lr=0.000265263\n","step 124: loss=2.818559, accuracy=0.2900, lr=0.000264998\n","step 125: loss=2.813413, accuracy=0.2900, lr=0.000264733\n","step 126: loss=2.808267, accuracy=0.3000, lr=0.000264468\n","step 127: loss=2.803048, accuracy=0.3000, lr=0.000264203\n","step 128: loss=2.797842, accuracy=0.3000, lr=0.000263939\n","step 129: loss=2.792531, accuracy=0.3000, lr=0.000263675\n","step 130: loss=2.787112, accuracy=0.3000, lr=0.000263411\n","step 131: loss=2.781587, accuracy=0.3100, lr=0.000263148\n","step 132: loss=2.775965, accuracy=0.3200, lr=0.000262885\n","step 133: loss=2.770245, accuracy=0.3200, lr=0.000262622\n","step 134: loss=2.764494, accuracy=0.3200, lr=0.000262359\n","step 135: loss=2.758812, accuracy=0.3200, lr=0.000262097\n","step 136: loss=2.753195, accuracy=0.3200, lr=0.000261835\n","step 137: loss=2.747486, accuracy=0.3100, lr=0.000261573\n","step 138: loss=2.741768, accuracy=0.3100, lr=0.000261312\n","step 139: loss=2.736083, accuracy=0.3300, lr=0.00026105\n","step 140: loss=2.730350, accuracy=0.3300, lr=0.000260789\n","step 141: loss=2.724493, accuracy=0.3300, lr=0.000260528\n","step 142: loss=2.718546, accuracy=0.3400, lr=0.000260268\n","step 143: loss=2.712459, accuracy=0.3200, lr=0.000260008\n","step 144: loss=2.706381, accuracy=0.3300, lr=0.000259748\n","step 145: loss=2.700338, accuracy=0.3400, lr=0.000259488\n","step 146: loss=2.694373, accuracy=0.3400, lr=0.000259228\n","step 147: loss=2.688412, accuracy=0.3500, lr=0.000258969\n","step 148: loss=2.682517, accuracy=0.3500, lr=0.00025871\n","step 149: loss=2.676695, accuracy=0.3600, lr=0.000258451\n","step 150: loss=2.670983, accuracy=0.3900, lr=0.000258193\n","step 151: loss=2.665303, accuracy=0.4000, lr=0.000257935\n","step 152: loss=2.659303, accuracy=0.4000, lr=0.000257677\n","step 153: loss=2.653147, accuracy=0.4000, lr=0.000257419\n","step 154: loss=2.646966, accuracy=0.4000, lr=0.000257162\n","step 155: loss=2.640979, accuracy=0.4000, lr=0.000256905\n","step 156: loss=2.635075, accuracy=0.4100, lr=0.000256648\n","step 157: loss=2.629222, accuracy=0.4100, lr=0.000256391\n","step 158: loss=2.623348, accuracy=0.4000, lr=0.000256135\n","step 159: loss=2.617378, accuracy=0.4000, lr=0.000255879\n","step 160: loss=2.611192, accuracy=0.4000, lr=0.000255623\n","step 161: loss=2.604811, accuracy=0.4100, lr=0.000255367\n","step 162: loss=2.598422, accuracy=0.4200, lr=0.000255112\n","step 163: loss=2.592049, accuracy=0.4300, lr=0.000254857\n","step 164: loss=2.585642, accuracy=0.4400, lr=0.000254602\n","step 165: loss=2.579357, accuracy=0.4400, lr=0.000254347\n","step 166: loss=2.573117, accuracy=0.4500, lr=0.000254093\n","step 167: loss=2.566878, accuracy=0.4400, lr=0.000253839\n","step 168: loss=2.560667, accuracy=0.4400, lr=0.000253585\n","step 169: loss=2.554399, accuracy=0.4400, lr=0.000253331\n","step 170: loss=2.548041, accuracy=0.4400, lr=0.000253078\n","step 171: loss=2.541960, accuracy=0.4400, lr=0.000252825\n","step 172: loss=2.535793, accuracy=0.4400, lr=0.000252572\n","step 173: loss=2.529409, accuracy=0.4400, lr=0.000252319\n","step 174: loss=2.523009, accuracy=0.4500, lr=0.000252067\n","step 175: loss=2.516662, accuracy=0.4700, lr=0.000251815\n","step 176: loss=2.510293, accuracy=0.4700, lr=0.000251563\n","step 177: loss=2.503889, accuracy=0.4700, lr=0.000251312\n","step 178: loss=2.497520, accuracy=0.4700, lr=0.00025106\n","step 179: loss=2.491055, accuracy=0.4700, lr=0.000250809\n","step 180: loss=2.484586, accuracy=0.4600, lr=0.000250558\n","step 181: loss=2.478206, accuracy=0.4600, lr=0.000250308\n","step 182: loss=2.471693, accuracy=0.4500, lr=0.000250058\n","step 183: loss=2.464979, accuracy=0.4500, lr=0.000249808\n","step 184: loss=2.458235, accuracy=0.4500, lr=0.000249558\n","step 185: loss=2.451548, accuracy=0.4500, lr=0.000249308\n","step 186: loss=2.444815, accuracy=0.4600, lr=0.000249059\n","step 187: loss=2.438042, accuracy=0.4600, lr=0.00024881\n","step 188: loss=2.431337, accuracy=0.4600, lr=0.000248561\n","step 189: loss=2.424680, accuracy=0.4700, lr=0.000248312\n","step 190: loss=2.417918, accuracy=0.4800, lr=0.000248064\n","step 191: loss=2.411321, accuracy=0.4800, lr=0.000247816\n","step 192: loss=2.404785, accuracy=0.4900, lr=0.000247568\n","step 193: loss=2.398194, accuracy=0.4900, lr=0.000247321\n","step 194: loss=2.391481, accuracy=0.4900, lr=0.000247073\n","step 195: loss=2.384651, accuracy=0.4900, lr=0.000246826\n","step 196: loss=2.377677, accuracy=0.4900, lr=0.000246579\n","step 197: loss=2.370607, accuracy=0.4900, lr=0.000246333\n","step 198: loss=2.363689, accuracy=0.4800, lr=0.000246087\n","step 199: loss=2.356944, accuracy=0.4900, lr=0.00024584\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"_5tKJ60r_75x"},"source":["# ---------------------------------------------------------------- Test ---------------------------------------------------------------- "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49uohqRJ_z4r","outputId":"2aaa2b6c-8ff3-45c8-e6b7-ccd42bde98aa"},"source":["data_dir = \"./images/test/\"\n","net = LeNet()\n","net.load(\"./lenet.npy\")\n","files = os.listdir(data_dir)\n","print(\"files: \", files)\n","images = []\n","labels = []\n","for f in files:\n","    img = cv2.imread(os.path.join(data_dir, f), cv2.IMREAD_GRAYSCALE)\n","    img = cv2.resize(img, (32, 32))\n","    img = img.astype(np.float32).reshape(32, 32, 1) / 255.0\n","    images.append(img)\n","\n","    if \"bear\" in f:\n","        num = 0\n","    elif \"leopard\" in f:\n","        num = 1\n","    elif \"tiger\" in f:\n","        num = 2\n","    elif \"dog\" in f:\n","        num = 3\n","    elif \"cat\" in f:\n","        num = 4\n","    elif \"lion\" in f:\n","        num = 5\n","    elif \"fox\" in f:\n","        num = 6\n","    elif \"polar_bear\" in f:\n","        num = 7\n","    elif \"meerkat\" in f:\n","        num = 8\n","    elif \"wolf\" in f:\n","        num = 9\n","\n","    #labels.append(int(f[0]))\n","    labels.append(num)\n","\n","x = np.array(images)\n","y = np.array(labels)\n","\n","predict = net.predict(x)\n","tp = np.sum(predict == y)\n","accuracy = float(tp) / len(files)\n","print(\"accuracy=%f\" % accuracy)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["files:  ['888_leopard.JPEG', '887_leopard.JPEG', '890_leopard.JPEG', '889_leopard.JPEG', '891_leopard.JPEG', '889_bear.JPEG', '890_bear.JPEG', '891_bear.JPEG', '888_bear.JPEG', '887_bear.JPEG', '888_meerkat.JPEG', '891_meerkat.JPEG', '890_meerkat.JPEG', '889_meerkat.JPEG', '887_meerkat.JPEG', '888_tiger.JPEG', '889_tiger.JPEG', '891_tiger.JPEG', '890_tiger.JPEG', '887_tiger.JPEG', '889_wolf.JPEG', '888_wolf.JPEG', '891_wolf.JPEG', '890_wolf.JPEG', '887_wolf.JPEG', '889_fox.JPEG', '887_fox.JPEG', '888_fox.JPEG', '891_fox.JPEG', '890_fox.JPEG', '887_lion.JPEG', '891_lion.JPEG', '889_lion.JPEG', '888_lion.JPEG', '890_lion.JPEG', '889_polar_bear.JPEG', '891_polar_bear.JPEG', '888_polar_bear.JPEG', '890_polar_bear.JPEG', '887_polar_bear.JPEG', '890_dog.JPEG', '888_dog.JPEG', '891_dog.JPEG', '889_dog.JPEG', '887_dog.JPEG', '889_cat.JPEG', '887_cat.JPEG', '891_cat.JPEG', '888_cat.JPEG', '890_cat.JPEG']\n"],"name":"stdout"}]}]}